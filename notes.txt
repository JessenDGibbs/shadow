
Docker commands:

Build: 
docker build -t shadow .   

Clear the containers: 
docker stop shadow-container
docker rm shadow-container

Create the container:
docker run -d -p 5001:5000 --name shadow-container -v shadow-volume:/app/public shadow


DN records for website:
75.2.70.75


/collect:

# Utility function to convert images to PDF
def image_to_pdf(image_path):
    image = Image.open(image_path)
    pdf_bytes = io.BytesIO()
    image.save(pdf_bytes, format="PDF")
    pdf_bytes.seek(0)
    return pdf_bytes

# Utility function to merge PDFs
def merge_pdfs(pdfs):
    merged_pdf = fitz.open()
    for pdf in pdfs:
        merged_pdf.insert_pdf(fitz.open(stream=pdf, filetype="pdf"))
    output = io.BytesIO()
    merged_pdf.save(output)
    output.seek(0)
    return output

@app.route('/collect', methods=['POST'])
def collect_files():
    if 'files' not in request.files:
        return "No files part in the request", 400

    files = request.files.getlist('files')
    temp_files = []  # To keep track of temporary files
    pdf_streams = []

    for file in files:
        if file.filename.endswith(('.jpg', '.jpeg', '.png')):
            # Convert image to PDF and add to the list
            pdf_stream = image_to_pdf(file)
            pdf_streams.append(pdf_stream)
        elif file.filename.endswith('.pdf'):
            # Directly add PDF stream to the list
            pdf_streams.append(file.stream)
        else:
            return "Unsupported file type", 400

    # Merge all PDF streams into a single PDF
    merged_pdf_stream = merge_pdfs(pdf_streams)

    # Cleanup any temporary files
    for temp_file in temp_files:
        os.remove(temp_file)

    # Return the merged PDF
    return send_file(merged_pdf_stream, attachment_filename="merged.pdf", as_attachment=True)


Updates for the Notion connection:
Just set up a Shadow page in Notion, get the pageID upload it once and save in long-term storage. 
If the value is not null don't present the add Id option and just show a sync button to pull all the
content from that page. Check if a pageID is present in Chroma, if not add it, if yes update the current
record with the current versino in case there have been updates

Do the same for Dropbox


An open-source API building personal internets and training detatchable agents that mimic your cognition. 
Personal internets and agents can be published to a marketplace to be licesned by anyone for a small fee.
Any personal agent can be attached to any internet to crawl it the way you would.

I believe in the value of the thought process, not just the thought itself. Although a research paper
delivers the key value that most are interested in, the process behind it carries untold value that
is often lost once the paper is out. Monetizing processeses has not been done and I believe both creators
and consumers will benefit from it greatly. The amount of thouhgt that wen't into building this app has
led me to deliver what it is now, any only to deliver the app since that is my core goal. However, the 
information I have aggregated through this process carries enourmous value for anyone who wants to:
build a react app, understand different API architectures, deploy a python app on Docker, migrate
Docker to GCP, build custom React components, use ngrok to test local api's on the web, etc. The internet
behind this app alone could benefit many and I pains me to know that others will have to go through
this same process over and over again because no one will take the time, rightfully, to think about how
to track and make sense of everything they go through so that it can bring value beyond their core goal.
The two main problems are 1) when we have a goal we only care about that goal and everything else is
irrelevant along the way. 2) even if I were to collect everything and tell you about it it would be
impossible for you to navigate and it would only be told through my perspective.

Solutions:
1) don't curate, just outline.
2) Build detatched cognitations that can be personalized and help you crawl my information in your way.

